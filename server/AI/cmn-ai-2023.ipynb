{"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO0fM6U/+eavVxNTfJFRpKE","provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Checking which GPU is alocated","metadata":{"id":"l8O_6dQHMqx-"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1181,"status":"ok","timestamp":1673997519548,"user":{"displayName":"Tiago Gomes","userId":"12810980286118356952"},"user_tz":0},"id":"U2tx1GCEMb60","outputId":"75495e1d-b1d4-4c92-f5d3-e0c7657ba85c","execution":{"iopub.status.busy":"2023-01-26T18:08:54.210469Z","iopub.execute_input":"2023-01-26T18:08:54.210915Z","iopub.status.idle":"2023-01-26T18:08:55.253467Z","shell.execute_reply.started":"2023-01-26T18:08:54.210804Z","shell.execute_reply":"2023-01-26T18:08:55.252271Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Thu Jan 26 18:08:55 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   32C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<hr>\n\n# Define imports and variables","metadata":{"id":"s2HnF2nXM9BB"}},{"cell_type":"code","source":"#!pip install tensorflow-addons\n\n#!pip install librosa\n\nimport tensorflow as tf\nprint(\"Tensorflow Version: \", tf.__version__)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7638,"status":"ok","timestamp":1673997545307,"user":{"displayName":"Tiago Gomes","userId":"12810980286118356952"},"user_tz":0},"id":"PuGPzmKmPVN5","outputId":"bd5dc102-2026-4ec1-ef25-9237d37de3d0","execution":{"iopub.status.busy":"2023-01-26T18:08:55.256366Z","iopub.execute_input":"2023-01-26T18:08:55.256798Z","iopub.status.idle":"2023-01-26T18:08:59.390247Z","shell.execute_reply.started":"2023-01-26T18:08:55.256756Z","shell.execute_reply":"2023-01-26T18:08:59.388347Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Tensorflow Version:  2.6.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport math\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport librosa \nfrom glob import glob\n\nimport random\nfrom functools import partial\nimport warnings\nwarnings.filterwarnings(\"ignore\")               # ignore some warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'        # ignore some warnings\n\nimport IPython.display as ipd\nfrom tensorflow.keras import mixed_precision","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1673997537675,"user":{"displayName":"Tiago Gomes","userId":"12810980286118356952"},"user_tz":0},"id":"qVtSNoNUML1z","execution":{"iopub.status.busy":"2023-01-26T18:08:59.391791Z","iopub.execute_input":"2023-01-26T18:08:59.392451Z","iopub.status.idle":"2023-01-26T18:09:01.694540Z","shell.execute_reply.started":"2023-01-26T18:08:59.392410Z","shell.execute_reply":"2023-01-26T18:09:01.693387Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n# Code","metadata":{}},{"cell_type":"code","source":"# GLOBAL VARIABLES\nmin_signal_rate = 0.02\nmax_signal_rate = 0.95\nema = 0.999\n\n# AUXILIARY FUNCTIONS TO DECLARE\ndef sinusoidal_embedding(x):\n    embedding_min_frequency = 1.0\n    embedding_max_frequency = 1000.0\n    embedding_dims = 32\n    frequencies = tf.exp(\n        tf.linspace(\n            tf.math.log(embedding_min_frequency),\n            tf.math.log(embedding_max_frequency),\n            embedding_dims // 2,\n        )\n    )\n    angular_speeds = 2.0 * math.pi * frequencies\n    embeddings = tf.concat(\n        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n    )\n    return embeddings\n\ndef ResidualBlock(width):\n    def apply(x):\n        input_width = x.shape[3]\n        if input_width == width:\n            residual = x\n        else:\n            residual = layers.Conv2D(width, kernel_size=1)(x)\n        x = layers.BatchNormalization(center=False, scale=False)(x)\n        x = layers.Conv2D(\n            width, kernel_size=3, padding=\"same\", activation=keras.activations.swish\n        )(x)\n        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n        x = layers.Add()([x, residual])\n        return x\n\n    return apply\n\ndef DownBlock(width, block_depth):\n    def apply(x):\n        x, skips = x\n        for _ in range(block_depth):\n            x = ResidualBlock(width)(x)\n            skips.append(x)\n        x = layers.AveragePooling2D(pool_size=2)(x)\n        return x\n\n    return apply\n\ndef UpBlock(width, block_depth, attention=False):\n    def apply(x):\n        x, skips = x\n        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n        for _ in range(block_depth):\n            skip = skips.pop()\n            x = layers.Concatenate()([x, skip] if not attention else [\n                x, skip, layers.MultiHeadAttention(\n                    num_heads=4, key_dim=1, attention_axes=(1,2)\n                )(x, skip)\n            ])\n            x = ResidualBlock(width)(x)\n        return x\n\n    return apply\n\ndef get_network(widths, block_depth, dim1=256, dim2=128, channels=1, attention=False):\n    noisy_input = keras.Input(shape=(dim1, dim2, channels))\n    noise_variances = keras.Input(shape=(1, 1, 1))\n    \n    upsample_shape = (dim1, dim2)\n\n    e = layers.Lambda(sinusoidal_embedding)(noise_variances)\n    e = layers.UpSampling2D(size=upsample_shape, interpolation=\"nearest\")(e)\n\n    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_input)\n    x = layers.Concatenate()([x, e])\n\n    skips = []\n    for width in widths[:-1]:\n        x = DownBlock(width, block_depth)([x, skips])\n\n    for _ in range(block_depth):\n        x = ResidualBlock(widths[-1])(x)\n\n    for idx, width in enumerate(reversed(widths[:-1])):\n        x = UpBlock(width, block_depth, attention=attention and idx ==0)([x, skips])\n\n    x = layers.Conv2D(channels, kernel_size=1, kernel_initializer=\"zeros\")(x)\n\n    return keras.Model([noisy_input, noise_variances], x, name=\"residual_unet\")\n\ndef spectral_norm(pred, real):\n    \"\"\"Calculate difference in spectral norm between two batches of spectrograms.\"\"\"\n    norm_real = tf.norm(real, axis=(1,2)) + 1e-6\n    norm_pred = tf.norm(pred, axis=(1,2)) + 1e-6\n    return tf.reduce_mean(tf.abs(norm_real - norm_pred) / norm_real)\n\ndef time_derivative(pred, real, window=1):\n    real_derivative = real[:, :-window, :, :] - real[:, window:, :, :]\n    pred_derivative = pred[:, :-window, :, :] - pred[:, window:, :, :]\n    return tf.reduce_mean(tf.keras.losses.MSE(real_derivative, pred_derivative))\n\n# CNN model that will be used\nclass DDIM(keras.Model):\n    \"\"\"DDIM model modified from this tutorial: https://keras.io/examples/generative/ddim/\"\"\"\n    \n    def __init__(self, widths, block_depth, attention=False, dim1=256, dim2=128):\n        super().__init__()\n\n        self.normalizer = layers.Normalization(axis=(2,3))\n        self.network = get_network(widths, block_depth, attention=attention, dim1=dim1, dim2=dim2)\n        self.ema_network = keras.models.clone_model(self.network)\n        self.spec_mod = 0\n        self.dx_mod = 0\n\n    def compile(self, **kwargs):\n        super().compile(**kwargs)\n\n        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n        self.data_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n        \n        self.noise_spec_tracker = keras.metrics.Mean(name=\"n_spec\")\n        self.data_spec_tracker = keras.metrics.Mean(name=\"d_spec\")\n        \n        self.noise_dx_tracker = keras.metrics.Mean(name=\"n_dx\")\n        self.data_dx_tracker = keras.metrics.Mean(name=\"d_dx\")\n        \n        self.noise_total_tracker = keras.metrics.Mean(name=\"n_total\")\n        self.data_total_tracker = keras.metrics.Mean(name=\"d_total\")\n\n    @property\n    def metrics(self):\n        return [\n            self.noise_loss_tracker, \n            self.data_loss_tracker,\n            \n            self.noise_spec_tracker,\n            self.data_spec_tracker,\n            \n            self.noise_dx_tracker,\n            self.data_dx_tracker,\n            \n            self.noise_total_tracker,\n            self.data_total_tracker\n        ]\n    \n    def update_trackers(self, n_l, n_s, n_d, d_l, d_s, d_d):\n        \"\"\"Update all loss trackers.\"\"\"\n        n_t = n_l + n_s + n_d\n        d_t = d_l + d_s + d_d\n        \n        for loss, tracker in zip(\n            [n_l, n_s, n_d, n_t, d_l, d_s, d_d, d_t], \n            [\n                self.noise_loss_tracker, self.noise_spec_tracker, self.noise_dx_tracker, self.noise_total_tracker,\n                self.data_loss_tracker, self.data_spec_tracker, self.data_dx_tracker, self.data_total_tracker\n            ]\n        ):\n            tracker.update_state(loss)\n            \n    def get_losses(self, y_true, y_pred):\n        \"\"\"Get losses for model.\"\"\"\n        return (\n            tf.reduce_mean(\n                self.loss(y_pred, y_true)\n            ), spectral_norm(\n                y_pred, y_true\n            ), time_derivative(\n                y_pred, y_true\n            )\n        )\n\n    def denormalize(self, data):\n        data = self.normalizer.mean + data * self.normalizer.variance**0.5\n        return tf.clip_by_value(data, -128.0, 128.0)\n\n    def diffusion_schedule(self, diffusion_times):\n        start_angle = tf.acos(max_signal_rate)\n        end_angle = tf.acos(min_signal_rate)\n        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n        signal_rates = tf.cos(diffusion_angles)\n        noise_rates = tf.sin(diffusion_angles)\n        return noise_rates, signal_rates\n\n    def denoise(self, noisy_data, noise_rates, signal_rates, training):\n        if training:\n            network = self.network\n        else:\n            network = self.ema_network\n        pred_noises = network([noisy_data, noise_rates**2], training=training)\n        pred_data = (noisy_data - noise_rates * pred_noises) / signal_rates\n\n        return pred_noises, pred_data\n\n    def reverse_diffusion(self, initial_noise, diffusion_steps):\n        num_examples = tf.shape(initial_noise)[0]\n        step_size = 1.0 / diffusion_steps\n\n        # important line:\n        # at the first sampling step, the \"noisy data\" is pure noise\n        # but its signal rate is assumed to be nonzero (min_signal_rate)\n        next_noisy_data = initial_noise\n        for step in tqdm(range(diffusion_steps)):\n            noisy_data = next_noisy_data\n\n            # separate the current noisy data to its components\n            diffusion_times = tf.ones((num_examples, 1, 1, 1)) - step * step_size\n            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n            pred_noises, pred_data = self.denoise(\n                noisy_data, noise_rates, signal_rates, training=False\n            )\n            # network used in eval mode\n\n            # remix the predicted components using the next signal and noise rates\n            next_diffusion_times = diffusion_times - step_size\n            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n                next_diffusion_times\n            )\n            next_noisy_data = (\n                next_signal_rates * pred_data + next_noise_rates * pred_noises\n            )\n            # this new noisy data will be used in the next step\n\n        return pred_data\n\n    def generate(self, num_examples, shape, diffusion_steps):\n        # noise -> data -> denormalized data\n        initial_noise = tf.random.normal(shape=(num_examples, shape[0], shape[1], shape[2]))\n        generated_data = self.reverse_diffusion(initial_noise, diffusion_steps)\n        generated_data = self.denormalize(generated_data)\n        return generated_data\n\n    def train_step(self, data):\n        batch_size = tf.shape(data)[0]\n        # normalize data to have standard deviation of 1, like the noises\n        data = self.normalizer(data, training=True)\n        noises = tf.random.normal(shape=tf.shape(data))\n\n        # sample uniform random diffusion times\n        diffusion_times = tf.random.uniform(\n            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n        )\n        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n        noise_rates = noise_rates\n        signal_rates = signal_rates\n        # mix the data with noises accordingly\n        noisy_data = signal_rates * data + noise_rates * noises\n\n        with tf.GradientTape() as tape:\n            # train the network to separate noisy data to their components\n            pred_noises, pred_data = self.denoise(\n                noisy_data, noise_rates, signal_rates, training=True\n            )\n\n            noise_loss, noise_spec, noise_dx = self.get_losses(noises, pred_noises) #safe_reduce_mean(self.loss(noises, pred_noises))  # used for training\n            total_noise_loss = tf.reduce_sum([\n                noise_loss, \n                self.spec_mod*noise_spec, \n                self.dx_mod*noise_dx\n            ])\n            data_loss, data_spec, data_dx = self.get_losses(data, pred_data) #safe_reduce_mean(self.loss(data, pred_data))  # only used as metric\n\n        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n\n        self.update_trackers(\n            noise_loss, noise_spec, noise_dx,\n            data_loss, data_spec, data_dx\n        )\n\n        # track the exponential moving averages of weights\n        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n\n        # KID is not measured during the training phase for computational efficiency\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        # normalize data to have standard deviation of 1, like the noises\n        batch_size = tf.shape(data)[0]\n        \n        data = self.normalizer(data, training=False)\n        noises = tf.random.normal(shape=tf.shape(data))\n\n        # sample uniform random diffusion times\n        diffusion_times = tf.random.uniform(\n            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n        )\n        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n        # mix the data with noises accordingly\n        noisy_data = signal_rates * data + noise_rates * noises\n\n        # use the network to separate noisy data to their components\n        pred_noises, pred_data = self.denoise(\n            noisy_data, noise_rates, signal_rates, training=False\n        )\n\n        noise_loss = self.loss(noises, pred_noises)\n        data_loss = self.loss(data, pred_data)\n\n        self.data_loss_tracker.update_state(data_loss)\n        self.noise_loss_tracker.update_state(noise_loss)\n\n        return {m.name: m.result() for m in self.metrics}\n\ndef load_at_interval(x, rate=10_000, feats=256, duration=3.3):\n    \"\"\"Load music from file at some offset. Return MDCT spectrogram of that data\"\"\"\n    file = x[0].numpy().decode()\n    idx = x[1].numpy()\n    audio, sr = librosa.load(file, duration=duration, sr=rate, offset=idx)\n    audio_fill = np.zeros(int(rate*duration), dtype=np.float32)\n    audio_fill[:len(audio)] = audio\n    spec = tf.signal.mdct(audio_fill, feats)\n    return spec\n\ndef load_audio(x,y, rate=10_000, mdct_feats=256, duration=3.3):\n    \"\"\"TF function for loading MDCT spectrogram from file.\"\"\"\n    out = tf.py_function(lambda x,y: load_at_interval( \n        (x,y), rate=rate, feats=mdct_feats, duration=duration\n    ), inp=[x,y], Tout=tf.float32)\n    return out\n\ndef get_files_dataset(\n        glob_location,\n        total_seconds=2,\n        out_len = 3.3,\n        hop_size=1,\n        max_feats = 2048,\n        batch_size=4,\n        shuffer_size=1000,\n        scale=1,\n        rate=10_000,\n        mdct_feats=256\n    ):\n    \"\"\"Get file dataset loader for a glob of audio files.\"\"\"\n    \n    files = glob(\n        glob_location,\n        recursive=True\n    )\n    \n#     files = [file for file in files if file not in exclude]\n    \n    def file_list_generator():\n        for _ in range(total_seconds):\n            for file in files:\n                yield file, _*hop_size\n                \n    load_fn = partial(load_audio, duration=out_len, rate=rate, mdct_feats=mdct_feats)\n                \n    dg =tf.data.Dataset.from_generator(\n        file_list_generator, output_signature = (\n            tf.TensorSpec(shape=(), dtype=tf.string), \n            tf.TensorSpec(shape=(), dtype=tf.int32)\n            )).shuffle(shuffer_size).map(\n                load_fn, num_parallel_calls=tf.data.AUTOTUNE).map(\n                    lambda x: tf.expand_dims(x, -1)[:max_feats, :, :]*scale).map(\n                        lambda x: tf.ensure_shape(x, (max_feats, mdct_feats//2, 1))).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    \n    return dg\n\ndef get_model_memory_usage(batch_size, model):\n    import numpy as np\n    try:\n        from keras import backend as K\n    except:\n        from tensorflow.keras import backend as K\n\n    shapes_mem_count = 0\n    internal_model_mem_count = 0\n    for l in model.layers:\n        layer_type = l.__class__.__name__\n        if layer_type == 'Model':\n            internal_model_mem_count += get_model_memory_usage(batch_size, l)\n        single_layer_mem = 1\n        out_shape = l.output_shape\n        if type(out_shape) is list:\n            out_shape = out_shape[0]\n        for s in out_shape:\n            if s is None:\n                continue\n            single_layer_mem *= s\n        shapes_mem_count += single_layer_mem\n\n    trainable_count = np.sum([K.count_params(p) for p in model.trainable_weights])\n    non_trainable_count = np.sum([K.count_params(p) for p in model.non_trainable_weights])\n\n    number_size = 4.0\n    if K.floatx() == 'float16':\n        number_size = 2.0\n    if K.floatx() == 'float64':\n        number_size = 8.0\n\n    total_memory = number_size * (batch_size * shapes_mem_count + trainable_count + non_trainable_count)\n    gbytes = np.round(total_memory / (1024.0 ** 3), 3) + internal_model_mem_count\n    return gbytes\n\n# one of the main functions:\n# metadata: array path for all the paths to retrieve the datasets\ndef train_ai (metadata, file_path, filename):\n    print(\"----> Metadata received: \", metadata)\n\n    # session configuration\n    session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\n    ####################################### GENERATE DATASET #######################################\n    # load musics with the metadata\n    TrackSet = []\n    \n    # iterate all the metadata paths and append the files to the list\n    for i in range(len(metadata)):\n        music_files = glob(metadata[i] + '/*.*.wav') # depending on file extension (avoids model weights files)... maybe proper dynamic file extension later?\n        dataset = get_files_dataset(\n            metadata[i] + '/*.*.wav',\n            out_len=3.3,\n            max_feats=256,\n            total_seconds=26,\n            scale=1,\n            batch_size=16\n        )\n\n        # @TODO: proper copy of whole array without creating matrix\n        for x in music_files:\n            TrackSet.append(x)\n    \n    #shape = dataset.take(1).shape\n    for test_batch in dataset.take(1):\n        shape = test_batch.shape\n\n    print(\"----> Shape: \", shape)\n\n    num_total_examples = (len(TrackSet))\n\n    print(\"----> Dataset size: \", num_total_examples)\n\n    ####################################### MODEL CREATION #######################################\n    # Model creation\n    print(\"----> Model Creation starting\")\n    model = DDIM(widths = [128, 128, 128, 128], block_depth = 2, attention=True, dim1=shape[1], dim2=shape[2])\n    model.normalizer.adapt(dataset)\n\n    model.compile(\n        loss=tf.keras.losses.MSE,\n        optimizer= tfa.optimizers.AdamW(\n            learning_rate = 3e-4,\n            weight_decay = 1e-4\n        )\n    )\n\n    dataset = dataset.cache()\n\n    ####################################### MODEL TRAINING #######################################\n    print(\"----> Model Training starting\")\n    \n    BATCH_SIZE = 8 # default is 32 by keras\n    nr_for_steps = num_total_examples / BATCH_SIZE # number for the steps per epoch should be this\n    \n    # using the same number as the original kaggle notebook (allows more steps to train)\n    kaggle_number = (num_total_examples * 26) // shape[0]\n    \n    # training first iteration\n    history = model.fit(dataset.repeat(), steps_per_epoch=kaggle_number, epochs=1)\n\n    # changing some stuff\n    model.spec_mod = 1\n    model.dx_mod = 1\n    \n    # train more 200 epochs after changing that\n    history = model.fit(dataset.repeat(), steps_per_epoch=kaggle_number, epochs=200)\n    \n    ###################################### SAMPLE GENERATION ######################################\n    import scipy.io.wavfile as wav\n    # generating 100 samples, 10 at a time\n    specs = model.generate(10, shape[1:], 1000)\n    # save the files on the folder\n    for i in range(len(specs)):\n        audio = tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32))\n        wav.write(file_path + \"/\" + filename + \"_0\" + str(i) + \".wav\", rate=10000, data=audio.numpy())\n    specs = model.generate(10, shape[1:], 1000)\n    # save the files on the folder\n    for i in range(len(specs)):\n        audio = tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32))\n        wav.write(file_path + \"/\" + filename + \"_1\" + str(i) + \".wav\", rate=10000, data=audio.numpy())\n    specs = model.generate(10, shape[1:], 1000)\n    # save the files on the folder\n    for i in range(len(specs)):\n        audio = tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32))\n        wav.write(file_path + \"/\" + filename + \"_2\" + str(i) + \".wav\", rate=10000, data=audio.numpy())\n    specs = model.generate(10, shape[1:], 1000)\n    # save the files on the folder\n    for i in range(len(specs)):\n        audio = tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32))\n        wav.write(file_path + \"/\" + filename + \"_3\" + str(i) + \".wav\", rate=10000, data=audio.numpy())\n    specs = model.generate(10, shape[1:], 1000)\n    # save the files on the folder\n    for i in range(len(specs)):\n        audio = tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32))\n        wav.write(file_path + \"/\" + filename + \"_4\" + str(i) + \".wav\", rate=10000, data=audio.numpy())\n    specs = model.generate(10, shape[1:], 1000)\n    # save the files on the folder\n    for i in range(len(specs)):\n        audio = tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32))\n        wav.write(file_path + \"/\" + filename + \"_5\" + str(i) + \".wav\", rate=10000, data=audio.numpy())\n    specs = model.generate(10, shape[1:], 1000)\n    # save the files on the folder\n    for i in range(len(specs)):\n        audio = tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32))\n        wav.write(file_path + \"/\" + filename + \"_6\" + str(i) + \".wav\", rate=10000, data=audio.numpy())\n    specs = model.generate(10, shape[1:], 1000)\n    # save the files on the folder\n    for i in range(len(specs)):\n        audio = tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32))\n        wav.write(file_path + \"/\" + filename + \"_7\" + str(i) + \".wav\", rate=10000, data=audio.numpy())\n    specs = model.generate(10, shape[1:], 1000)\n    # save the files on the folder\n    for i in range(len(specs)):\n        audio = tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32))\n        wav.write(file_path + \"/\" + filename + \"_8\" + str(i) + \".wav\", rate=10000, data=audio.numpy())\n    specs = model.generate(10, shape[1:], 1000)\n    # save the files on the folder\n    for i in range(len(specs)):\n        audio = tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32))\n        wav.write(file_path + \"/\" + filename + \"_9\" + str(i) + \".wav\", rate=10000, data=audio.numpy())\n    \n    # listen to 2 different samples on the notebook\n    #for i in range(2):\n    #    plt.pcolormesh(np.log(np.abs(specs[i, :, :, 0].numpy().T)))\n    #    plt.colorbar()\n    #    plt.title(f\"Generated example {i+1}\")\n    #    plt.show()\n    #    ipd.display(ipd.Audio(tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32)), rate=10_000))\n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-26T18:09:01.697035Z","iopub.execute_input":"2023-01-26T18:09:01.697433Z","iopub.status.idle":"2023-01-26T18:09:01.779389Z","shell.execute_reply.started":"2023-01-26T18:09:01.697390Z","shell.execute_reply":"2023-01-26T18:09:01.777377Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# generate full path to gather different datasets\ndef gen_metadata(inputs, metadata):\n    # define the metadata array\n    for i in range(len(inputs)):\n        # calculate the full path of the metadata\n        FULL_PATH = BASE_PATH + inputs[i]\n\n        metadata.append(FULL_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T18:09:01.783018Z","iopub.execute_input":"2023-01-26T18:09:01.783394Z","iopub.status.idle":"2023-01-26T18:09:01.798592Z","shell.execute_reply.started":"2023-01-26T18:09:01.783356Z","shell.execute_reply":"2023-01-26T18:09:01.797699Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# python function to check if a path exists\n# if it doesn’t exist we create one\n# assumes just one path inside metadata for now\n# @TODO: create a new path depending on multiple elements\ndef gen_output_folder(base_path, metadata):\n    # remove the BASE_PATH prefix\n    if metadata[0].startswith(BASE_PATH):\n        path = metadata[0][len(BASE_PATH):]\n        \n    # make dir\n    dir_path = base_path + path\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n        \n    return dir_path","metadata":{"execution":{"iopub.status.busy":"2023-01-26T18:09:01.800041Z","iopub.execute_input":"2023-01-26T18:09:01.800452Z","iopub.status.idle":"2023-01-26T18:09:01.808560Z","shell.execute_reply.started":"2023-01-26T18:09:01.800414Z","shell.execute_reply":"2023-01-26T18:09:01.807617Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n# Create different metadata for testing","metadata":{"id":"9I93S565WO7v"}},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input/cmndataset/cmn-dataset/\"\n\n# define metadata to gather different datasets (this way, it allows to merge different things)\ninput_metadata_1 = [\"Classical/All/All\"]\nmetadata_1 = []\n\ninput_metadata_2 = [\"Jazz/All/All\"]\nmetadata_2 = []\n\ninput_metadata_3 = [\"Rock/All/All\"]\nmetadata_3 = []\n\ninput_metadata_4 = [\"Disco/All/All\"]\nmetadata_4 = []\n\ninput_metadata_5 = [\"Blues/All/All\"]\nmetadata_5 = []\n\ninput_metadata_6 = [\"Pop/All/All\"]\nmetadata_6 = []\n\ninput_metadata_7 = [\"Reggae/All/All\"]\nmetadata_7 = []\n\ninput_metadata_8 = [\"Country/All/All\"]\nmetadata_8 = []\n\ninput_metadata_9 = [\"Metal/All/All\"]\nmetadata_9 = []\n\ninput_metadata_10 = [\"Hiphop/All/All\"]\nmetadata_10 = []\n\n\n# generated metadata for testing\ngen_metadata(input_metadata_1, metadata_1)\nprint(metadata_1)\n\ngen_metadata(input_metadata_2, metadata_2)\nprint(metadata_2)\n\ngen_metadata(input_metadata_3, metadata_3)\nprint(metadata_3)\n\ngen_metadata(input_metadata_4, metadata_4)\nprint(metadata_4)\n\ngen_metadata(input_metadata_5, metadata_5)\nprint(metadata_5)\n\ngen_metadata(input_metadata_6, metadata_6)\nprint(metadata_6)\n\ngen_metadata(input_metadata_7, metadata_7)\nprint(metadata_7)\n\ngen_metadata(input_metadata_8, metadata_8)\nprint(metadata_8)\n\ngen_metadata(input_metadata_9, metadata_9)\nprint(metadata_9)\n\ngen_metadata(input_metadata_10, metadata_10)\nprint(metadata_10)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1673997548034,"user":{"displayName":"Tiago Gomes","userId":"12810980286118356952"},"user_tz":0},"id":"Mpth3B4hPevn","outputId":"5202def0-507e-49b3-c089-628d4b4ee5a2","execution":{"iopub.status.busy":"2023-01-26T18:09:01.810086Z","iopub.execute_input":"2023-01-26T18:09:01.810619Z","iopub.status.idle":"2023-01-26T18:09:01.823699Z","shell.execute_reply.started":"2023-01-26T18:09:01.810583Z","shell.execute_reply":"2023-01-26T18:09:01.822613Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['/kaggle/input/cmndataset/cmn-dataset/Classical/All/All']\n['/kaggle/input/cmndataset/cmn-dataset/Jazz/All/All']\n['/kaggle/input/cmndataset/cmn-dataset/Rock/All/All']\n['/kaggle/input/cmndataset/cmn-dataset/Disco/All/All']\n['/kaggle/input/cmndataset/cmn-dataset/Blues/All/All']\n['/kaggle/input/cmndataset/cmn-dataset/Pop/All/All']\n['/kaggle/input/cmndataset/cmn-dataset/Reggae/All/All']\n['/kaggle/input/cmndataset/cmn-dataset/Country/All/All']\n['/kaggle/input/cmndataset/cmn-dataset/Metal/All/All']\n['/kaggle/input/cmndataset/cmn-dataset/Hiphop/All/All']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<hr>\n\n# Train the model and generate samples","metadata":{}},{"cell_type":"code","source":"BASE_PATH_GENERATED = \"/kaggle/working/\"\n\n# generate different file paths depending on the metadata\npath_1 = gen_output_folder(BASE_PATH_GENERATED, metadata_1)\npath_2 = gen_output_folder(BASE_PATH_GENERATED, metadata_2)\npath_3 = gen_output_folder(BASE_PATH_GENERATED, metadata_3)\npath_4 = gen_output_folder(BASE_PATH_GENERATED, metadata_4)\npath_5 = gen_output_folder(BASE_PATH_GENERATED, metadata_5)\npath_6 = gen_output_folder(BASE_PATH_GENERATED, metadata_6)\npath_7 = gen_output_folder(BASE_PATH_GENERATED, metadata_7)\npath_8 = gen_output_folder(BASE_PATH_GENERATED, metadata_8)\npath_9 = gen_output_folder(BASE_PATH_GENERATED, metadata_9)\npath_10 = gen_output_folder(BASE_PATH_GENERATED, metadata_10)\n\n# train and generate samples\n#train_ai(metadata_1, path_1, \"generated\")\n#train_ai(metadata_2, path_2, \"generated\")\ntrain_ai(metadata_3, path_3, \"generated\")\n#train_ai(metadata_4, path_4, \"generated\")\n#train_ai(metadata_5, path_5, \"generated\")\n#train_ai(metadata_6, path_6, \"generated\")\n#train_ai(metadata_7, path_7, \"generated\")\n#train_ai(metadata_8, path_8, \"generated\")\n#train_ai(metadata_9, path_9, \"generated\")\n#train_ai(metadata_10, path_10, \"generated\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7QPGFYa8RJ91","outputId":"03db7798-6513-42a3-ef83-cb2c04c58b87","execution":{"iopub.status.busy":"2023-01-26T18:09:01.825526Z","iopub.execute_input":"2023-01-26T18:09:01.826074Z","iopub.status.idle":"2023-01-27T01:39:27.400882Z","shell.execute_reply.started":"2023-01-26T18:09:01.826038Z","shell.execute_reply":"2023-01-27T01:39:27.399423Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"----> Metadata received:  ['/kaggle/input/cmndataset/cmn-dataset/Rock/All/All']\nDevice mapping:\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n\n----> Shape:  (16, 256, 128, 1)\n----> Dataset size:  100\n----> Model Creation starting\n----> Model Training starting\n","output_type":"stream"},{"name":"stderr","text":"2023-01-26 18:13:33.651961: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.71GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n","output_type":"stream"},{"name":"stdout","text":"162/162 [==============================] - 363s 2s/step - n_loss: 0.3012 - d_loss: 2.1312 - n_spec: 0.1964 - d_spec: 0.5673 - n_dx: 0.5990 - d_dx: 4.1566 - n_total: 1.0966 - d_total: 6.8550\n","output_type":"stream"},{"name":"stderr","text":"2023-01-26 18:19:21.180196: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n162/162 [==============================] - 349s 2s/step - n_loss: 0.2511 - d_loss: 0.7864 - n_spec: 0.1533 - d_spec: 0.3820 - n_dx: 0.4993 - d_dx: 1.4923 - n_total: 0.9038 - d_total: 2.6607\nEpoch 2/200\n162/162 [==============================] - 123s 750ms/step - n_loss: 0.2374 - d_loss: 0.6403 - n_spec: 0.1421 - d_spec: 0.3275 - n_dx: 0.4733 - d_dx: 1.2264 - n_total: 0.8529 - d_total: 2.1942\nEpoch 3/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2359 - d_loss: 0.6303 - n_spec: 0.1424 - d_spec: 0.3188 - n_dx: 0.4707 - d_dx: 1.2331 - n_total: 0.8490 - d_total: 2.1822\nEpoch 4/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2244 - d_loss: 0.5732 - n_spec: 0.1347 - d_spec: 0.3286 - n_dx: 0.4477 - d_dx: 1.1266 - n_total: 0.8068 - d_total: 2.0285\nEpoch 5/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2273 - d_loss: 0.5508 - n_spec: 0.1350 - d_spec: 0.3155 - n_dx: 0.4535 - d_dx: 1.0633 - n_total: 0.8158 - d_total: 1.9295\nEpoch 6/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2279 - d_loss: 0.5531 - n_spec: 0.1378 - d_spec: 0.2892 - n_dx: 0.4548 - d_dx: 1.0844 - n_total: 0.8205 - d_total: 1.9267\nEpoch 7/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2189 - d_loss: 0.5398 - n_spec: 0.1298 - d_spec: 0.3129 - n_dx: 0.4368 - d_dx: 1.0504 - n_total: 0.7855 - d_total: 1.9031\nEpoch 8/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2203 - d_loss: 0.5227 - n_spec: 0.1300 - d_spec: 0.3046 - n_dx: 0.4397 - d_dx: 1.0268 - n_total: 0.7900 - d_total: 1.8541\nEpoch 9/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2141 - d_loss: 0.5315 - n_spec: 0.1260 - d_spec: 0.3172 - n_dx: 0.4273 - d_dx: 1.0509 - n_total: 0.7674 - d_total: 1.8996\nEpoch 10/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2194 - d_loss: 0.5144 - n_spec: 0.1300 - d_spec: 0.3085 - n_dx: 0.4379 - d_dx: 1.0084 - n_total: 0.7872 - d_total: 1.8313\nEpoch 11/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2117 - d_loss: 0.5562 - n_spec: 0.1245 - d_spec: 0.3078 - n_dx: 0.4218 - d_dx: 1.0653 - n_total: 0.7579 - d_total: 1.9293\nEpoch 12/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2219 - d_loss: 0.4859 - n_spec: 0.1297 - d_spec: 0.3068 - n_dx: 0.4432 - d_dx: 0.9563 - n_total: 0.7948 - d_total: 1.7490\nEpoch 13/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2169 - d_loss: 0.5125 - n_spec: 0.1276 - d_spec: 0.3215 - n_dx: 0.4331 - d_dx: 1.0135 - n_total: 0.7777 - d_total: 1.8475\nEpoch 14/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2155 - d_loss: 0.5046 - n_spec: 0.1265 - d_spec: 0.3059 - n_dx: 0.4301 - d_dx: 0.9845 - n_total: 0.7721 - d_total: 1.7950\nEpoch 15/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2158 - d_loss: 0.5244 - n_spec: 0.1279 - d_spec: 0.3058 - n_dx: 0.4304 - d_dx: 1.0049 - n_total: 0.7741 - d_total: 1.8351\nEpoch 16/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2097 - d_loss: 0.5341 - n_spec: 0.1234 - d_spec: 0.3151 - n_dx: 0.4187 - d_dx: 1.0570 - n_total: 0.7518 - d_total: 1.9061\nEpoch 17/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2116 - d_loss: 0.4966 - n_spec: 0.1239 - d_spec: 0.2984 - n_dx: 0.4227 - d_dx: 0.9774 - n_total: 0.7582 - d_total: 1.7725\nEpoch 18/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2167 - d_loss: 0.4932 - n_spec: 0.1273 - d_spec: 0.3081 - n_dx: 0.4323 - d_dx: 0.9547 - n_total: 0.7762 - d_total: 1.7559\nEpoch 19/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2140 - d_loss: 0.5033 - n_spec: 0.1261 - d_spec: 0.2963 - n_dx: 0.4273 - d_dx: 0.9890 - n_total: 0.7674 - d_total: 1.7886\nEpoch 20/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2096 - d_loss: 0.4967 - n_spec: 0.1218 - d_spec: 0.3005 - n_dx: 0.4188 - d_dx: 0.9847 - n_total: 0.7501 - d_total: 1.7819\nEpoch 21/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2088 - d_loss: 0.4975 - n_spec: 0.1219 - d_spec: 0.2969 - n_dx: 0.4170 - d_dx: 0.9794 - n_total: 0.7477 - d_total: 1.7737\nEpoch 22/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2125 - d_loss: 0.5287 - n_spec: 0.1264 - d_spec: 0.3070 - n_dx: 0.4244 - d_dx: 1.0431 - n_total: 0.7634 - d_total: 1.8788\nEpoch 23/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2036 - d_loss: 0.5164 - n_spec: 0.1194 - d_spec: 0.3080 - n_dx: 0.4069 - d_dx: 1.0206 - n_total: 0.7299 - d_total: 1.8449\nEpoch 24/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2098 - d_loss: 0.4865 - n_spec: 0.1211 - d_spec: 0.2972 - n_dx: 0.4192 - d_dx: 0.9682 - n_total: 0.7502 - d_total: 1.7519\nEpoch 25/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2164 - d_loss: 0.4773 - n_spec: 0.1260 - d_spec: 0.2944 - n_dx: 0.4325 - d_dx: 0.9385 - n_total: 0.7749 - d_total: 1.7102\nEpoch 26/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2102 - d_loss: 0.4641 - n_spec: 0.1219 - d_spec: 0.3003 - n_dx: 0.4198 - d_dx: 0.9171 - n_total: 0.7518 - d_total: 1.6814\nEpoch 27/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2170 - d_loss: 0.4884 - n_spec: 0.1268 - d_spec: 0.2975 - n_dx: 0.4338 - d_dx: 0.9611 - n_total: 0.7776 - d_total: 1.7470\nEpoch 28/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2064 - d_loss: 0.4969 - n_spec: 0.1201 - d_spec: 0.3026 - n_dx: 0.4124 - d_dx: 0.9775 - n_total: 0.7388 - d_total: 1.7770\nEpoch 29/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2093 - d_loss: 0.4987 - n_spec: 0.1228 - d_spec: 0.2955 - n_dx: 0.4186 - d_dx: 0.9816 - n_total: 0.7507 - d_total: 1.7758\nEpoch 30/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2154 - d_loss: 0.4839 - n_spec: 0.1252 - d_spec: 0.2923 - n_dx: 0.4306 - d_dx: 0.9482 - n_total: 0.7713 - d_total: 1.7243\nEpoch 31/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2023 - d_loss: 0.4766 - n_spec: 0.1165 - d_spec: 0.3004 - n_dx: 0.4044 - d_dx: 0.9485 - n_total: 0.7232 - d_total: 1.7255\nEpoch 32/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2110 - d_loss: 0.4839 - n_spec: 0.1234 - d_spec: 0.3033 - n_dx: 0.4220 - d_dx: 0.9536 - n_total: 0.7564 - d_total: 1.7407\nEpoch 33/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2136 - d_loss: 0.4644 - n_spec: 0.1238 - d_spec: 0.2940 - n_dx: 0.4271 - d_dx: 0.9165 - n_total: 0.7646 - d_total: 1.6749\nEpoch 34/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2081 - d_loss: 0.4865 - n_spec: 0.1215 - d_spec: 0.2928 - n_dx: 0.4157 - d_dx: 0.9534 - n_total: 0.7453 - d_total: 1.7327\nEpoch 35/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2098 - d_loss: 0.4783 - n_spec: 0.1211 - d_spec: 0.2892 - n_dx: 0.4195 - d_dx: 0.9438 - n_total: 0.7504 - d_total: 1.7113\nEpoch 36/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2001 - d_loss: 0.4957 - n_spec: 0.1157 - d_spec: 0.3059 - n_dx: 0.4001 - d_dx: 0.9818 - n_total: 0.7158 - d_total: 1.7835\nEpoch 37/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2076 - d_loss: 0.4863 - n_spec: 0.1213 - d_spec: 0.2980 - n_dx: 0.4151 - d_dx: 0.9682 - n_total: 0.7440 - d_total: 1.7525\nEpoch 38/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2093 - d_loss: 0.4910 - n_spec: 0.1226 - d_spec: 0.2988 - n_dx: 0.4188 - d_dx: 0.9780 - n_total: 0.7507 - d_total: 1.7679\nEpoch 39/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2100 - d_loss: 0.4757 - n_spec: 0.1214 - d_spec: 0.2943 - n_dx: 0.4198 - d_dx: 0.9495 - n_total: 0.7512 - d_total: 1.7196\nEpoch 40/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.1960 - d_loss: 0.5059 - n_spec: 0.1134 - d_spec: 0.3067 - n_dx: 0.3922 - d_dx: 1.0010 - n_total: 0.7017 - d_total: 1.8137\nEpoch 41/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2045 - d_loss: 0.4689 - n_spec: 0.1176 - d_spec: 0.3021 - n_dx: 0.4090 - d_dx: 0.9278 - n_total: 0.7311 - d_total: 1.6987\nEpoch 42/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2040 - d_loss: 0.4677 - n_spec: 0.1181 - d_spec: 0.2976 - n_dx: 0.4081 - d_dx: 0.9292 - n_total: 0.7302 - d_total: 1.6945\nEpoch 43/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2086 - d_loss: 0.4808 - n_spec: 0.1218 - d_spec: 0.3006 - n_dx: 0.4173 - d_dx: 0.9647 - n_total: 0.7477 - d_total: 1.7462\nEpoch 44/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2074 - d_loss: 0.4739 - n_spec: 0.1212 - d_spec: 0.2969 - n_dx: 0.4147 - d_dx: 0.9375 - n_total: 0.7433 - d_total: 1.7083\nEpoch 45/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2086 - d_loss: 0.4613 - n_spec: 0.1206 - d_spec: 0.2894 - n_dx: 0.4171 - d_dx: 0.9164 - n_total: 0.7464 - d_total: 1.6671\nEpoch 46/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2123 - d_loss: 0.4871 - n_spec: 0.1241 - d_spec: 0.2824 - n_dx: 0.4241 - d_dx: 0.9486 - n_total: 0.7606 - d_total: 1.7182\nEpoch 47/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2072 - d_loss: 0.4650 - n_spec: 0.1206 - d_spec: 0.2924 - n_dx: 0.4146 - d_dx: 0.9290 - n_total: 0.7424 - d_total: 1.6864\nEpoch 48/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2114 - d_loss: 0.4600 - n_spec: 0.1226 - d_spec: 0.2934 - n_dx: 0.4228 - d_dx: 0.9163 - n_total: 0.7568 - d_total: 1.6697\nEpoch 49/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2072 - d_loss: 0.4834 - n_spec: 0.1205 - d_spec: 0.3057 - n_dx: 0.4142 - d_dx: 0.9507 - n_total: 0.7418 - d_total: 1.7399\nEpoch 50/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2069 - d_loss: 0.4839 - n_spec: 0.1197 - d_spec: 0.3055 - n_dx: 0.4140 - d_dx: 0.9603 - n_total: 0.7407 - d_total: 1.7497\nEpoch 51/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2100 - d_loss: 0.4570 - n_spec: 0.1212 - d_spec: 0.2945 - n_dx: 0.4198 - d_dx: 0.8942 - n_total: 0.7510 - d_total: 1.6457\nEpoch 52/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2035 - d_loss: 0.4860 - n_spec: 0.1181 - d_spec: 0.2974 - n_dx: 0.4068 - d_dx: 0.9514 - n_total: 0.7285 - d_total: 1.7349\nEpoch 53/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2058 - d_loss: 0.4710 - n_spec: 0.1194 - d_spec: 0.2995 - n_dx: 0.4114 - d_dx: 0.9346 - n_total: 0.7365 - d_total: 1.7051\nEpoch 54/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2039 - d_loss: 0.4752 - n_spec: 0.1186 - d_spec: 0.2935 - n_dx: 0.4076 - d_dx: 0.9433 - n_total: 0.7301 - d_total: 1.7120\nEpoch 55/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2083 - d_loss: 0.4679 - n_spec: 0.1210 - d_spec: 0.2931 - n_dx: 0.4163 - d_dx: 0.9228 - n_total: 0.7455 - d_total: 1.6838\nEpoch 56/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2042 - d_loss: 0.4800 - n_spec: 0.1180 - d_spec: 0.2940 - n_dx: 0.4084 - d_dx: 0.9484 - n_total: 0.7306 - d_total: 1.7224\nEpoch 57/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2071 - d_loss: 0.4809 - n_spec: 0.1197 - d_spec: 0.2920 - n_dx: 0.4140 - d_dx: 0.9469 - n_total: 0.7408 - d_total: 1.7197\nEpoch 58/200\n162/162 [==============================] - 121s 747ms/step - n_loss: 0.2027 - d_loss: 0.4923 - n_spec: 0.1186 - d_spec: 0.3021 - n_dx: 0.4049 - d_dx: 0.9559 - n_total: 0.7262 - d_total: 1.7504\nEpoch 59/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2100 - d_loss: 0.4485 - n_spec: 0.1220 - d_spec: 0.2896 - n_dx: 0.4198 - d_dx: 0.8913 - n_total: 0.7519 - d_total: 1.6294\nEpoch 60/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2052 - d_loss: 0.4683 - n_spec: 0.1194 - d_spec: 0.3010 - n_dx: 0.4106 - d_dx: 0.9295 - n_total: 0.7352 - d_total: 1.6987\nEpoch 61/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2112 - d_loss: 0.4662 - n_spec: 0.1230 - d_spec: 0.2858 - n_dx: 0.4222 - d_dx: 0.9219 - n_total: 0.7564 - d_total: 1.6739\nEpoch 62/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2067 - d_loss: 0.4632 - n_spec: 0.1198 - d_spec: 0.2943 - n_dx: 0.4133 - d_dx: 0.9147 - n_total: 0.7398 - d_total: 1.6722\nEpoch 63/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1980 - d_loss: 0.4959 - n_spec: 0.1146 - d_spec: 0.3012 - n_dx: 0.3956 - d_dx: 0.9716 - n_total: 0.7082 - d_total: 1.7688\nEpoch 64/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2012 - d_loss: 0.4709 - n_spec: 0.1165 - d_spec: 0.3004 - n_dx: 0.4022 - d_dx: 0.9316 - n_total: 0.7198 - d_total: 1.7030\nEpoch 65/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2038 - d_loss: 0.4567 - n_spec: 0.1177 - d_spec: 0.2909 - n_dx: 0.4075 - d_dx: 0.9085 - n_total: 0.7290 - d_total: 1.6561\nEpoch 66/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2045 - d_loss: 0.4637 - n_spec: 0.1182 - d_spec: 0.2924 - n_dx: 0.4089 - d_dx: 0.9162 - n_total: 0.7316 - d_total: 1.6723\nEpoch 67/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2114 - d_loss: 0.4440 - n_spec: 0.1232 - d_spec: 0.2871 - n_dx: 0.4227 - d_dx: 0.8770 - n_total: 0.7572 - d_total: 1.6081\nEpoch 68/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2078 - d_loss: 0.4696 - n_spec: 0.1204 - d_spec: 0.2865 - n_dx: 0.4154 - d_dx: 0.9213 - n_total: 0.7436 - d_total: 1.6774\nEpoch 69/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2050 - d_loss: 0.4811 - n_spec: 0.1196 - d_spec: 0.2934 - n_dx: 0.4096 - d_dx: 0.9532 - n_total: 0.7342 - d_total: 1.7277\nEpoch 70/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2009 - d_loss: 0.4935 - n_spec: 0.1172 - d_spec: 0.3029 - n_dx: 0.4017 - d_dx: 0.9781 - n_total: 0.7198 - d_total: 1.7745\nEpoch 71/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2007 - d_loss: 0.4610 - n_spec: 0.1158 - d_spec: 0.3057 - n_dx: 0.4012 - d_dx: 0.9185 - n_total: 0.7178 - d_total: 1.6852\nEpoch 72/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2063 - d_loss: 0.4439 - n_spec: 0.1191 - d_spec: 0.2924 - n_dx: 0.4125 - d_dx: 0.8834 - n_total: 0.7378 - d_total: 1.6197\nEpoch 73/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2019 - d_loss: 0.4743 - n_spec: 0.1160 - d_spec: 0.2939 - n_dx: 0.4036 - d_dx: 0.9363 - n_total: 0.7215 - d_total: 1.7045\nEpoch 74/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2031 - d_loss: 0.4607 - n_spec: 0.1167 - d_spec: 0.2911 - n_dx: 0.4059 - d_dx: 0.9098 - n_total: 0.7257 - d_total: 1.6617\nEpoch 75/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2033 - d_loss: 0.4707 - n_spec: 0.1183 - d_spec: 0.2925 - n_dx: 0.4064 - d_dx: 0.9332 - n_total: 0.7280 - d_total: 1.6964\nEpoch 76/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2095 - d_loss: 0.4628 - n_spec: 0.1217 - d_spec: 0.2837 - n_dx: 0.4189 - d_dx: 0.9163 - n_total: 0.7501 - d_total: 1.6629\nEpoch 77/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2027 - d_loss: 0.4658 - n_spec: 0.1173 - d_spec: 0.2961 - n_dx: 0.4050 - d_dx: 0.9177 - n_total: 0.7250 - d_total: 1.6796\nEpoch 78/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2046 - d_loss: 0.4615 - n_spec: 0.1184 - d_spec: 0.2989 - n_dx: 0.4089 - d_dx: 0.9123 - n_total: 0.7319 - d_total: 1.6726\nEpoch 79/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2065 - d_loss: 0.4524 - n_spec: 0.1188 - d_spec: 0.2875 - n_dx: 0.4127 - d_dx: 0.8985 - n_total: 0.7380 - d_total: 1.6384\nEpoch 80/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2034 - d_loss: 0.4625 - n_spec: 0.1169 - d_spec: 0.2961 - n_dx: 0.4065 - d_dx: 0.9111 - n_total: 0.7268 - d_total: 1.6697\nEpoch 81/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2037 - d_loss: 0.4539 - n_spec: 0.1172 - d_spec: 0.2953 - n_dx: 0.4073 - d_dx: 0.9021 - n_total: 0.7283 - d_total: 1.6514\nEpoch 82/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2062 - d_loss: 0.4455 - n_spec: 0.1186 - d_spec: 0.2920 - n_dx: 0.4120 - d_dx: 0.8801 - n_total: 0.7368 - d_total: 1.6176\nEpoch 83/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2002 - d_loss: 0.4775 - n_spec: 0.1158 - d_spec: 0.2938 - n_dx: 0.4000 - d_dx: 0.9386 - n_total: 0.7160 - d_total: 1.7098\nEpoch 84/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2006 - d_loss: 0.4608 - n_spec: 0.1164 - d_spec: 0.2952 - n_dx: 0.4012 - d_dx: 0.9178 - n_total: 0.7182 - d_total: 1.6738\nEpoch 85/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2032 - d_loss: 0.4576 - n_spec: 0.1173 - d_spec: 0.2973 - n_dx: 0.4063 - d_dx: 0.9031 - n_total: 0.7268 - d_total: 1.6580\nEpoch 86/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2064 - d_loss: 0.4670 - n_spec: 0.1203 - d_spec: 0.2950 - n_dx: 0.4125 - d_dx: 0.9273 - n_total: 0.7392 - d_total: 1.6894\nEpoch 87/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2083 - d_loss: 0.4523 - n_spec: 0.1211 - d_spec: 0.2920 - n_dx: 0.4165 - d_dx: 0.8995 - n_total: 0.7459 - d_total: 1.6438\nEpoch 88/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1985 - d_loss: 0.4753 - n_spec: 0.1149 - d_spec: 0.3022 - n_dx: 0.3968 - d_dx: 0.9441 - n_total: 0.7102 - d_total: 1.7216\nEpoch 89/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2021 - d_loss: 0.4695 - n_spec: 0.1164 - d_spec: 0.2904 - n_dx: 0.4036 - d_dx: 0.9207 - n_total: 0.7221 - d_total: 1.6806\nEpoch 90/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2039 - d_loss: 0.4695 - n_spec: 0.1191 - d_spec: 0.2934 - n_dx: 0.4077 - d_dx: 0.9328 - n_total: 0.7307 - d_total: 1.6957\nEpoch 91/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2030 - d_loss: 0.4527 - n_spec: 0.1176 - d_spec: 0.2891 - n_dx: 0.4058 - d_dx: 0.8975 - n_total: 0.7263 - d_total: 1.6393\nEpoch 92/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2054 - d_loss: 0.4569 - n_spec: 0.1189 - d_spec: 0.2868 - n_dx: 0.4104 - d_dx: 0.9013 - n_total: 0.7347 - d_total: 1.6451\nEpoch 93/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2031 - d_loss: 0.4562 - n_spec: 0.1168 - d_spec: 0.2963 - n_dx: 0.4062 - d_dx: 0.9021 - n_total: 0.7262 - d_total: 1.6546\nEpoch 94/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1981 - d_loss: 0.4677 - n_spec: 0.1135 - d_spec: 0.3012 - n_dx: 0.3957 - d_dx: 0.9236 - n_total: 0.7072 - d_total: 1.6925\nEpoch 95/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1996 - d_loss: 0.4642 - n_spec: 0.1145 - d_spec: 0.3003 - n_dx: 0.3990 - d_dx: 0.9173 - n_total: 0.7130 - d_total: 1.6819\nEpoch 96/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2049 - d_loss: 0.4621 - n_spec: 0.1182 - d_spec: 0.2930 - n_dx: 0.4094 - d_dx: 0.9062 - n_total: 0.7325 - d_total: 1.6613\nEpoch 97/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1952 - d_loss: 0.4820 - n_spec: 0.1127 - d_spec: 0.2989 - n_dx: 0.3899 - d_dx: 0.9551 - n_total: 0.6979 - d_total: 1.7360\nEpoch 98/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2054 - d_loss: 0.4525 - n_spec: 0.1187 - d_spec: 0.2942 - n_dx: 0.4106 - d_dx: 0.8931 - n_total: 0.7348 - d_total: 1.6398\nEpoch 99/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2016 - d_loss: 0.4673 - n_spec: 0.1161 - d_spec: 0.3060 - n_dx: 0.4031 - d_dx: 0.9270 - n_total: 0.7208 - d_total: 1.7003\nEpoch 100/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2073 - d_loss: 0.4679 - n_spec: 0.1211 - d_spec: 0.2909 - n_dx: 0.4141 - d_dx: 0.9232 - n_total: 0.7425 - d_total: 1.6819\nEpoch 101/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2021 - d_loss: 0.4656 - n_spec: 0.1169 - d_spec: 0.2975 - n_dx: 0.4040 - d_dx: 0.9225 - n_total: 0.7230 - d_total: 1.6856\nEpoch 102/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2067 - d_loss: 0.4427 - n_spec: 0.1189 - d_spec: 0.2976 - n_dx: 0.4130 - d_dx: 0.8771 - n_total: 0.7385 - d_total: 1.6174\nEpoch 103/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2013 - d_loss: 0.4574 - n_spec: 0.1166 - d_spec: 0.2888 - n_dx: 0.4023 - d_dx: 0.9047 - n_total: 0.7202 - d_total: 1.6509\nEpoch 104/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2025 - d_loss: 0.4601 - n_spec: 0.1169 - d_spec: 0.2924 - n_dx: 0.4046 - d_dx: 0.9106 - n_total: 0.7240 - d_total: 1.6631\nEpoch 105/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2009 - d_loss: 0.4505 - n_spec: 0.1153 - d_spec: 0.2953 - n_dx: 0.4017 - d_dx: 0.8965 - n_total: 0.7178 - d_total: 1.6423\nEpoch 106/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2001 - d_loss: 0.4559 - n_spec: 0.1155 - d_spec: 0.2916 - n_dx: 0.4001 - d_dx: 0.9024 - n_total: 0.7158 - d_total: 1.6499\nEpoch 107/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1982 - d_loss: 0.4674 - n_spec: 0.1138 - d_spec: 0.3032 - n_dx: 0.3960 - d_dx: 0.9249 - n_total: 0.7080 - d_total: 1.6954\nEpoch 108/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1978 - d_loss: 0.4771 - n_spec: 0.1138 - d_spec: 0.3113 - n_dx: 0.3956 - d_dx: 0.9469 - n_total: 0.7071 - d_total: 1.7352\nEpoch 109/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1970 - d_loss: 0.4583 - n_spec: 0.1126 - d_spec: 0.2906 - n_dx: 0.3937 - d_dx: 0.9081 - n_total: 0.7033 - d_total: 1.6570\nEpoch 110/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2036 - d_loss: 0.4724 - n_spec: 0.1182 - d_spec: 0.2931 - n_dx: 0.4067 - d_dx: 0.9227 - n_total: 0.7285 - d_total: 1.6881\nEpoch 111/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2050 - d_loss: 0.4493 - n_spec: 0.1192 - d_spec: 0.2879 - n_dx: 0.4098 - d_dx: 0.8956 - n_total: 0.7341 - d_total: 1.6327\nEpoch 112/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2051 - d_loss: 0.4622 - n_spec: 0.1185 - d_spec: 0.2883 - n_dx: 0.4097 - d_dx: 0.9152 - n_total: 0.7332 - d_total: 1.6658\nEpoch 113/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2022 - d_loss: 0.4571 - n_spec: 0.1165 - d_spec: 0.2927 - n_dx: 0.4041 - d_dx: 0.9037 - n_total: 0.7228 - d_total: 1.6535\nEpoch 114/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2049 - d_loss: 0.4590 - n_spec: 0.1185 - d_spec: 0.2989 - n_dx: 0.4094 - d_dx: 0.9090 - n_total: 0.7328 - d_total: 1.6669\nEpoch 115/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2025 - d_loss: 0.4531 - n_spec: 0.1166 - d_spec: 0.2953 - n_dx: 0.4045 - d_dx: 0.8923 - n_total: 0.7236 - d_total: 1.6407\nEpoch 116/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2034 - d_loss: 0.4491 - n_spec: 0.1166 - d_spec: 0.2939 - n_dx: 0.4070 - d_dx: 0.8916 - n_total: 0.7270 - d_total: 1.6346\nEpoch 117/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2067 - d_loss: 0.4457 - n_spec: 0.1197 - d_spec: 0.2921 - n_dx: 0.4129 - d_dx: 0.8781 - n_total: 0.7393 - d_total: 1.6158\nEpoch 118/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1997 - d_loss: 0.4816 - n_spec: 0.1158 - d_spec: 0.3000 - n_dx: 0.3991 - d_dx: 0.9522 - n_total: 0.7146 - d_total: 1.7339\nEpoch 119/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1992 - d_loss: 0.4629 - n_spec: 0.1147 - d_spec: 0.2958 - n_dx: 0.3982 - d_dx: 0.9179 - n_total: 0.7120 - d_total: 1.6765\nEpoch 120/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1965 - d_loss: 0.4648 - n_spec: 0.1130 - d_spec: 0.2973 - n_dx: 0.3928 - d_dx: 0.9201 - n_total: 0.7022 - d_total: 1.6823\nEpoch 121/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2008 - d_loss: 0.4561 - n_spec: 0.1161 - d_spec: 0.2941 - n_dx: 0.4014 - d_dx: 0.9057 - n_total: 0.7183 - d_total: 1.6559\nEpoch 122/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2000 - d_loss: 0.4606 - n_spec: 0.1150 - d_spec: 0.2921 - n_dx: 0.3999 - d_dx: 0.9142 - n_total: 0.7148 - d_total: 1.6669\nEpoch 123/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.1986 - d_loss: 0.4782 - n_spec: 0.1146 - d_spec: 0.3027 - n_dx: 0.3967 - d_dx: 0.9494 - n_total: 0.7099 - d_total: 1.7302\nEpoch 124/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2007 - d_loss: 0.4522 - n_spec: 0.1151 - d_spec: 0.2933 - n_dx: 0.4012 - d_dx: 0.8990 - n_total: 0.7170 - d_total: 1.6445\nEpoch 125/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1988 - d_loss: 0.4637 - n_spec: 0.1152 - d_spec: 0.2915 - n_dx: 0.3971 - d_dx: 0.9156 - n_total: 0.7112 - d_total: 1.6708\nEpoch 126/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2068 - d_loss: 0.4352 - n_spec: 0.1190 - d_spec: 0.2915 - n_dx: 0.4133 - d_dx: 0.8661 - n_total: 0.7391 - d_total: 1.5928\nEpoch 127/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2002 - d_loss: 0.4522 - n_spec: 0.1155 - d_spec: 0.2951 - n_dx: 0.4001 - d_dx: 0.8956 - n_total: 0.7158 - d_total: 1.6429\nEpoch 128/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1998 - d_loss: 0.4640 - n_spec: 0.1156 - d_spec: 0.2930 - n_dx: 0.3993 - d_dx: 0.9246 - n_total: 0.7146 - d_total: 1.6816\nEpoch 129/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2052 - d_loss: 0.4569 - n_spec: 0.1183 - d_spec: 0.2874 - n_dx: 0.4103 - d_dx: 0.9037 - n_total: 0.7338 - d_total: 1.6480\nEpoch 130/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1944 - d_loss: 0.4795 - n_spec: 0.1116 - d_spec: 0.2986 - n_dx: 0.3887 - d_dx: 0.9529 - n_total: 0.6947 - d_total: 1.7311\nEpoch 131/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2064 - d_loss: 0.4518 - n_spec: 0.1194 - d_spec: 0.2783 - n_dx: 0.4124 - d_dx: 0.8926 - n_total: 0.7383 - d_total: 1.6228\nEpoch 132/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1985 - d_loss: 0.4592 - n_spec: 0.1141 - d_spec: 0.2988 - n_dx: 0.3969 - d_dx: 0.9085 - n_total: 0.7095 - d_total: 1.6666\nEpoch 133/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2067 - d_loss: 0.4345 - n_spec: 0.1185 - d_spec: 0.2826 - n_dx: 0.4133 - d_dx: 0.8615 - n_total: 0.7386 - d_total: 1.5787\nEpoch 134/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2037 - d_loss: 0.4440 - n_spec: 0.1173 - d_spec: 0.2981 - n_dx: 0.4072 - d_dx: 0.8796 - n_total: 0.7282 - d_total: 1.6217\nEpoch 135/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2033 - d_loss: 0.4437 - n_spec: 0.1166 - d_spec: 0.2864 - n_dx: 0.4063 - d_dx: 0.8792 - n_total: 0.7262 - d_total: 1.6094\nEpoch 136/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2022 - d_loss: 0.4531 - n_spec: 0.1165 - d_spec: 0.2986 - n_dx: 0.4042 - d_dx: 0.8932 - n_total: 0.7229 - d_total: 1.6449\nEpoch 137/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1999 - d_loss: 0.4873 - n_spec: 0.1169 - d_spec: 0.3048 - n_dx: 0.3993 - d_dx: 0.9650 - n_total: 0.7161 - d_total: 1.7571\nEpoch 138/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2017 - d_loss: 0.4657 - n_spec: 0.1160 - d_spec: 0.2891 - n_dx: 0.4025 - d_dx: 0.9081 - n_total: 0.7202 - d_total: 1.6628\nEpoch 139/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1959 - d_loss: 0.4521 - n_spec: 0.1124 - d_spec: 0.3012 - n_dx: 0.3916 - d_dx: 0.8959 - n_total: 0.6999 - d_total: 1.6491\nEpoch 140/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2000 - d_loss: 0.4628 - n_spec: 0.1153 - d_spec: 0.2877 - n_dx: 0.3998 - d_dx: 0.9221 - n_total: 0.7151 - d_total: 1.6727\nEpoch 141/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2026 - d_loss: 0.4462 - n_spec: 0.1169 - d_spec: 0.2913 - n_dx: 0.4050 - d_dx: 0.8826 - n_total: 0.7245 - d_total: 1.6201\nEpoch 142/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2003 - d_loss: 0.4592 - n_spec: 0.1153 - d_spec: 0.2960 - n_dx: 0.4006 - d_dx: 0.9073 - n_total: 0.7162 - d_total: 1.6625\nEpoch 143/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1988 - d_loss: 0.4476 - n_spec: 0.1146 - d_spec: 0.2911 - n_dx: 0.3973 - d_dx: 0.8871 - n_total: 0.7106 - d_total: 1.6259\nEpoch 144/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1953 - d_loss: 0.4624 - n_spec: 0.1128 - d_spec: 0.3008 - n_dx: 0.3901 - d_dx: 0.9113 - n_total: 0.6982 - d_total: 1.6746\nEpoch 145/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2062 - d_loss: 0.4385 - n_spec: 0.1193 - d_spec: 0.2916 - n_dx: 0.4123 - d_dx: 0.8676 - n_total: 0.7378 - d_total: 1.5977\nEpoch 146/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1935 - d_loss: 0.4735 - n_spec: 0.1106 - d_spec: 0.3001 - n_dx: 0.3867 - d_dx: 0.9310 - n_total: 0.6908 - d_total: 1.7046\nEpoch 147/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2018 - d_loss: 0.4412 - n_spec: 0.1155 - d_spec: 0.2882 - n_dx: 0.4034 - d_dx: 0.8780 - n_total: 0.7208 - d_total: 1.6073\nEpoch 148/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1959 - d_loss: 0.4700 - n_spec: 0.1134 - d_spec: 0.3003 - n_dx: 0.3915 - d_dx: 0.9258 - n_total: 0.7008 - d_total: 1.6961\nEpoch 149/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2020 - d_loss: 0.4497 - n_spec: 0.1160 - d_spec: 0.2883 - n_dx: 0.4035 - d_dx: 0.8841 - n_total: 0.7215 - d_total: 1.6221\nEpoch 150/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2012 - d_loss: 0.4633 - n_spec: 0.1157 - d_spec: 0.2937 - n_dx: 0.4023 - d_dx: 0.9198 - n_total: 0.7192 - d_total: 1.6768\nEpoch 151/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2060 - d_loss: 0.4549 - n_spec: 0.1193 - d_spec: 0.2895 - n_dx: 0.4118 - d_dx: 0.9022 - n_total: 0.7371 - d_total: 1.6466\nEpoch 152/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1984 - d_loss: 0.4586 - n_spec: 0.1145 - d_spec: 0.2967 - n_dx: 0.3965 - d_dx: 0.9104 - n_total: 0.7094 - d_total: 1.6657\nEpoch 153/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1964 - d_loss: 0.4707 - n_spec: 0.1129 - d_spec: 0.2955 - n_dx: 0.3925 - d_dx: 0.9280 - n_total: 0.7018 - d_total: 1.6941\nEpoch 154/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2051 - d_loss: 0.4535 - n_spec: 0.1184 - d_spec: 0.2948 - n_dx: 0.4098 - d_dx: 0.8918 - n_total: 0.7333 - d_total: 1.6401\nEpoch 155/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.1999 - d_loss: 0.4634 - n_spec: 0.1154 - d_spec: 0.2923 - n_dx: 0.3996 - d_dx: 0.9207 - n_total: 0.7149 - d_total: 1.6764\nEpoch 156/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2024 - d_loss: 0.4472 - n_spec: 0.1169 - d_spec: 0.2934 - n_dx: 0.4043 - d_dx: 0.8870 - n_total: 0.7236 - d_total: 1.6277\nEpoch 157/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2058 - d_loss: 0.4362 - n_spec: 0.1190 - d_spec: 0.2818 - n_dx: 0.4112 - d_dx: 0.8567 - n_total: 0.7360 - d_total: 1.5747\nEpoch 158/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2017 - d_loss: 0.4427 - n_spec: 0.1159 - d_spec: 0.2962 - n_dx: 0.4031 - d_dx: 0.8791 - n_total: 0.7207 - d_total: 1.6180\nEpoch 159/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2024 - d_loss: 0.4439 - n_spec: 0.1161 - d_spec: 0.2947 - n_dx: 0.4046 - d_dx: 0.8833 - n_total: 0.7231 - d_total: 1.6219\nEpoch 160/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1995 - d_loss: 0.4613 - n_spec: 0.1157 - d_spec: 0.2917 - n_dx: 0.3987 - d_dx: 0.9166 - n_total: 0.7139 - d_total: 1.6696\nEpoch 161/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1937 - d_loss: 0.4671 - n_spec: 0.1108 - d_spec: 0.2985 - n_dx: 0.3872 - d_dx: 0.9270 - n_total: 0.6917 - d_total: 1.6925\nEpoch 162/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2032 - d_loss: 0.4507 - n_spec: 0.1183 - d_spec: 0.2917 - n_dx: 0.4061 - d_dx: 0.8913 - n_total: 0.7276 - d_total: 1.6337\nEpoch 163/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2074 - d_loss: 0.4353 - n_spec: 0.1196 - d_spec: 0.2877 - n_dx: 0.4145 - d_dx: 0.8617 - n_total: 0.7416 - d_total: 1.5847\nEpoch 164/200\n162/162 [==============================] - 122s 750ms/step - n_loss: 0.1989 - d_loss: 0.4550 - n_spec: 0.1144 - d_spec: 0.2902 - n_dx: 0.3977 - d_dx: 0.9045 - n_total: 0.7110 - d_total: 1.6497\nEpoch 165/200\n162/162 [==============================] - 121s 750ms/step - n_loss: 0.1985 - d_loss: 0.4603 - n_spec: 0.1144 - d_spec: 0.2948 - n_dx: 0.3963 - d_dx: 0.9027 - n_total: 0.7092 - d_total: 1.6578\nEpoch 166/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1999 - d_loss: 0.4486 - n_spec: 0.1146 - d_spec: 0.2960 - n_dx: 0.3996 - d_dx: 0.8915 - n_total: 0.7141 - d_total: 1.6362\nEpoch 167/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1938 - d_loss: 0.4674 - n_spec: 0.1109 - d_spec: 0.2980 - n_dx: 0.3874 - d_dx: 0.9253 - n_total: 0.6921 - d_total: 1.6907\nEpoch 168/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1986 - d_loss: 0.4624 - n_spec: 0.1149 - d_spec: 0.2940 - n_dx: 0.3969 - d_dx: 0.9151 - n_total: 0.7104 - d_total: 1.6716\nEpoch 169/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2048 - d_loss: 0.4398 - n_spec: 0.1178 - d_spec: 0.2876 - n_dx: 0.4094 - d_dx: 0.8719 - n_total: 0.7321 - d_total: 1.5993\nEpoch 170/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2009 - d_loss: 0.4671 - n_spec: 0.1155 - d_spec: 0.2855 - n_dx: 0.4014 - d_dx: 0.9226 - n_total: 0.7179 - d_total: 1.6753\nEpoch 171/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2027 - d_loss: 0.4621 - n_spec: 0.1180 - d_spec: 0.2949 - n_dx: 0.4052 - d_dx: 0.9129 - n_total: 0.7259 - d_total: 1.6699\nEpoch 172/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1994 - d_loss: 0.4739 - n_spec: 0.1141 - d_spec: 0.2966 - n_dx: 0.3982 - d_dx: 0.9160 - n_total: 0.7117 - d_total: 1.6865\nEpoch 173/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2021 - d_loss: 0.4433 - n_spec: 0.1168 - d_spec: 0.2922 - n_dx: 0.4038 - d_dx: 0.8778 - n_total: 0.7227 - d_total: 1.6133\nEpoch 174/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2024 - d_loss: 0.4660 - n_spec: 0.1174 - d_spec: 0.2910 - n_dx: 0.4046 - d_dx: 0.9217 - n_total: 0.7244 - d_total: 1.6787\nEpoch 175/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2062 - d_loss: 0.4477 - n_spec: 0.1196 - d_spec: 0.2926 - n_dx: 0.4121 - d_dx: 0.8859 - n_total: 0.7379 - d_total: 1.6261\nEpoch 176/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2026 - d_loss: 0.4441 - n_spec: 0.1165 - d_spec: 0.2939 - n_dx: 0.4049 - d_dx: 0.8850 - n_total: 0.7240 - d_total: 1.6230\nEpoch 177/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1978 - d_loss: 0.4638 - n_spec: 0.1140 - d_spec: 0.2905 - n_dx: 0.3952 - d_dx: 0.9149 - n_total: 0.7071 - d_total: 1.6691\nEpoch 178/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2019 - d_loss: 0.4579 - n_spec: 0.1166 - d_spec: 0.2864 - n_dx: 0.4032 - d_dx: 0.9063 - n_total: 0.7217 - d_total: 1.6506\nEpoch 179/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1991 - d_loss: 0.4607 - n_spec: 0.1150 - d_spec: 0.2983 - n_dx: 0.3975 - d_dx: 0.9069 - n_total: 0.7116 - d_total: 1.6659\nEpoch 180/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1948 - d_loss: 0.4505 - n_spec: 0.1117 - d_spec: 0.2991 - n_dx: 0.3894 - d_dx: 0.8957 - n_total: 0.6958 - d_total: 1.6453\nEpoch 181/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1973 - d_loss: 0.4544 - n_spec: 0.1134 - d_spec: 0.3034 - n_dx: 0.3943 - d_dx: 0.8987 - n_total: 0.7051 - d_total: 1.6565\nEpoch 182/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2032 - d_loss: 0.4422 - n_spec: 0.1165 - d_spec: 0.2894 - n_dx: 0.4060 - d_dx: 0.8776 - n_total: 0.7257 - d_total: 1.6093\nEpoch 183/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2016 - d_loss: 0.4500 - n_spec: 0.1162 - d_spec: 0.2909 - n_dx: 0.4024 - d_dx: 0.8768 - n_total: 0.7202 - d_total: 1.6177\nEpoch 184/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1986 - d_loss: 0.4701 - n_spec: 0.1152 - d_spec: 0.2954 - n_dx: 0.3969 - d_dx: 0.9348 - n_total: 0.7106 - d_total: 1.7003\nEpoch 185/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2036 - d_loss: 0.4249 - n_spec: 0.1165 - d_spec: 0.2900 - n_dx: 0.4069 - d_dx: 0.8448 - n_total: 0.7269 - d_total: 1.5597\nEpoch 186/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1993 - d_loss: 0.4420 - n_spec: 0.1144 - d_spec: 0.3005 - n_dx: 0.3982 - d_dx: 0.8764 - n_total: 0.7119 - d_total: 1.6188\nEpoch 187/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2009 - d_loss: 0.4418 - n_spec: 0.1149 - d_spec: 0.2880 - n_dx: 0.4015 - d_dx: 0.8782 - n_total: 0.7172 - d_total: 1.6080\nEpoch 188/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1979 - d_loss: 0.4534 - n_spec: 0.1133 - d_spec: 0.2882 - n_dx: 0.3951 - d_dx: 0.8963 - n_total: 0.7063 - d_total: 1.6378\nEpoch 189/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2068 - d_loss: 0.4316 - n_spec: 0.1192 - d_spec: 0.2872 - n_dx: 0.4132 - d_dx: 0.8582 - n_total: 0.7392 - d_total: 1.5770\nEpoch 190/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1963 - d_loss: 0.4551 - n_spec: 0.1125 - d_spec: 0.2953 - n_dx: 0.3922 - d_dx: 0.9015 - n_total: 0.7010 - d_total: 1.6518\nEpoch 191/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1958 - d_loss: 0.4509 - n_spec: 0.1120 - d_spec: 0.2930 - n_dx: 0.3911 - d_dx: 0.8894 - n_total: 0.6989 - d_total: 1.6333\nEpoch 192/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1996 - d_loss: 0.4599 - n_spec: 0.1157 - d_spec: 0.2925 - n_dx: 0.3987 - d_dx: 0.9092 - n_total: 0.7140 - d_total: 1.6615\nEpoch 193/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2031 - d_loss: 0.4424 - n_spec: 0.1168 - d_spec: 0.2904 - n_dx: 0.4059 - d_dx: 0.8735 - n_total: 0.7258 - d_total: 1.6063\nEpoch 194/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.1986 - d_loss: 0.4509 - n_spec: 0.1140 - d_spec: 0.2930 - n_dx: 0.3969 - d_dx: 0.8903 - n_total: 0.7095 - d_total: 1.6342\nEpoch 195/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.1972 - d_loss: 0.4599 - n_spec: 0.1132 - d_spec: 0.2946 - n_dx: 0.3940 - d_dx: 0.9054 - n_total: 0.7044 - d_total: 1.6599\nEpoch 196/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2028 - d_loss: 0.4471 - n_spec: 0.1168 - d_spec: 0.2904 - n_dx: 0.4054 - d_dx: 0.8915 - n_total: 0.7250 - d_total: 1.6290\nEpoch 197/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2035 - d_loss: 0.4435 - n_spec: 0.1169 - d_spec: 0.2883 - n_dx: 0.4066 - d_dx: 0.8808 - n_total: 0.7270 - d_total: 1.6126\nEpoch 198/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.1991 - d_loss: 0.4549 - n_spec: 0.1151 - d_spec: 0.2952 - n_dx: 0.3977 - d_dx: 0.8991 - n_total: 0.7119 - d_total: 1.6493\nEpoch 199/200\n162/162 [==============================] - 121s 749ms/step - n_loss: 0.2003 - d_loss: 0.4504 - n_spec: 0.1146 - d_spec: 0.2897 - n_dx: 0.4002 - d_dx: 0.8957 - n_total: 0.7151 - d_total: 1.6359\nEpoch 200/200\n162/162 [==============================] - 121s 748ms/step - n_loss: 0.2012 - d_loss: 0.4495 - n_spec: 0.1157 - d_spec: 0.3027 - n_dx: 0.4019 - d_dx: 0.8888 - n_total: 0.7188 - d_total: 1.6410\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [03:14<00:00,  5.14it/s]\n100%|██████████| 1000/1000 [03:11<00:00,  5.23it/s]\n100%|██████████| 1000/1000 [03:11<00:00,  5.23it/s]\n100%|██████████| 1000/1000 [03:11<00:00,  5.23it/s]\n100%|██████████| 1000/1000 [03:11<00:00,  5.23it/s]\n100%|██████████| 1000/1000 [03:11<00:00,  5.23it/s]\n100%|██████████| 1000/1000 [03:11<00:00,  5.23it/s]\n100%|██████████| 1000/1000 [03:11<00:00,  5.23it/s]\n100%|██████████| 1000/1000 [03:11<00:00,  5.23it/s]\n100%|██████████| 1000/1000 [03:11<00:00,  5.23it/s]\n","output_type":"stream"}]}]}